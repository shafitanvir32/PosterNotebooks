{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9859880,"sourceType":"datasetVersion","datasetId":6051134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install transformers if not already installed\n!pip install transformers\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport random\nimport time\nimport datetime\nimport os\n\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:57:37.114470Z","iopub.execute_input":"2024-11-10T08:57:37.115088Z","iopub.status.idle":"2024-11-10T08:57:56.286473Z","shell.execute_reply.started":"2024-11-10T08:57:37.115043Z","shell.execute_reply":"2024-11-10T08:57:56.285500Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/banfake2/BanFake.csv')\n\n# Check data\nprint(df.head())\nprint('Number of samples:', df.shape[0])\n\n# Check label distribution\nprint(df['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:58:46.729285Z","iopub.execute_input":"2024-11-10T08:58:46.730224Z","iopub.status.idle":"2024-11-10T08:58:52.487167Z","shell.execute_reply.started":"2024-11-10T08:58:46.730183Z","shell.execute_reply":"2024-11-10T08:58:52.486211Z"}},"outputs":[{"name":"stdout","text":"   label                                               news\n0      1  হট্টগোল করায় বাকৃবিতে দুইজন বরখাস্ত, ৬ জনকে শো...\n1      1  মালয়েশিয়ায় কর্মী পাঠানোর ব্যবস্থা নেয়ার সুপারি...\n2      1  প্রেমের প্রস্তাবে রাজি না হওয়ায় স্কুলছাত্রীকে ...\n3      1  মেডিয়েশনই মামলাজট নিরসনের পথ : বিচারপতি আহমেদ ...\n4      1  টকশোতে বক্তব্য দিতে গিয়ে জাপা নেতার মৃত্যু মাদ...\nNumber of samples: 49977\nlabel\n1    48678\n0     1299\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Split into training and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n\nprint('Training set size:', train_df.shape[0])\nprint('Validation set size:', val_df.shape[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:58:57.269412Z","iopub.execute_input":"2024-11-10T08:58:57.270131Z","iopub.status.idle":"2024-11-10T08:58:57.306464Z","shell.execute_reply.started":"2024-11-10T08:58:57.270091Z","shell.execute_reply":"2024-11-10T08:58:57.305592Z"}},"outputs":[{"name":"stdout","text":"Training set size: 44979\nValidation set size: 4998\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:25:40.135252Z","iopub.execute_input":"2024-11-10T09:25:40.135674Z","iopub.status.idle":"2024-11-10T09:25:41.687949Z","shell.execute_reply.started":"2024-11-10T09:25:40.135634Z","shell.execute_reply":"2024-11-10T09:25:41.687125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c992b296796c42738aba405f878bebca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f84801c340479a9205cdb36fbe0ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c29a8b81a4e472c9b2e47c8756e7247"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18c1b9d469549c8b61bd0b86f596ff2"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def tokenize_and_split(text, tokenizer, max_length):\n    # Tokenize the text\n    tokens = tokenizer.tokenize(text)\n    \n    # Account for [CLS] and [SEP] with \"- 2\"\n    if len(tokens) > max_length - 2:\n        # Split the tokens into chunks of size max_length - 2\n        chunks = [tokens[i:i + (max_length - 2)] for i in range(0, len(tokens), max_length - 2)]\n    else:\n        chunks = [tokens]\n    \n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:25:58.991219Z","iopub.execute_input":"2024-11-10T09:25:58.991636Z","iopub.status.idle":"2024-11-10T09:25:58.997422Z","shell.execute_reply.started":"2024-11-10T09:25:58.991595Z","shell.execute_reply":"2024-11-10T09:25:58.996517Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def prepare_data_with_chunking(df, tokenizer, max_length):\n    input_ids = []\n    attention_masks = []\n    labels = []\n    article_ids = []\n    \n    for idx, row in df.iterrows():\n        text = row['news']\n        label = row['label']\n        \n        chunks = tokenize_and_split(text, tokenizer, max_length)\n        \n        for chunk in chunks:\n            # Reconstruct the chunk back to a string\n            chunk_text = tokenizer.convert_tokens_to_string(chunk)\n            \n            # Encode the chunk\n            encoding = tokenizer.encode_plus(\n                chunk_text,\n                add_special_tokens=True,\n                max_length=max_length,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_tensors='pt',\n            )\n            \n            input_ids.append(encoding['input_ids'])\n            attention_masks.append(encoding['attention_mask'])\n            labels.append(label)\n            article_ids.append(idx)  # Keep track of the article ID\n    \n    # Convert lists to tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n    article_ids = torch.tensor(article_ids)\n    \n    return input_ids, attention_masks, labels, article_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:26:16.630310Z","iopub.execute_input":"2024-11-10T09:26:16.631142Z","iopub.status.idle":"2024-11-10T09:26:16.642761Z","shell.execute_reply.started":"2024-11-10T09:26:16.631101Z","shell.execute_reply":"2024-11-10T09:26:16.641793Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"MAX_LEN = 512  # Maximum sequence length for BERT\n\n# Prepare training data\ntrain_input_ids, train_attention_masks, train_labels, train_article_ids = prepare_data_with_chunking(train_df, tokenizer, MAX_LEN)\n\n# Prepare validation data\nval_input_ids, val_attention_masks, val_labels, val_article_ids = prepare_data_with_chunking(val_df, tokenizer, MAX_LEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:26:18.571141Z","iopub.execute_input":"2024-11-10T09:26:18.571928Z","iopub.status.idle":"2024-11-10T09:47:28.122434Z","shell.execute_reply.started":"2024-11-10T09:26:18.571891Z","shell.execute_reply":"2024-11-10T09:47:28.121343Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\n# Create the datasets\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels, train_article_ids)\nval_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels, val_article_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T09:59:53.142108Z","iopub.execute_input":"2024-11-10T09:59:53.143036Z","iopub.status.idle":"2024-11-10T09:59:53.151639Z","shell.execute_reply.started":"2024-11-10T09:59:53.142992Z","shell.execute_reply":"2024-11-10T09:59:53.150702Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Set batch size\nbatch_size = 24  # Adjust based on GPU memory\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:02:47.543473Z","iopub.execute_input":"2024-11-10T10:02:47.544309Z","iopub.status.idle":"2024-11-10T10:02:47.549667Z","shell.execute_reply.started":"2024-11-10T10:02:47.544266Z","shell.execute_reply":"2024-11-10T10:02:47.548687Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# Initialize the model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n\n# Move model to the default device\nmodel.to(device)\n\n# Wrap the model with DataParallel\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:03:30.205798Z","iopub.execute_input":"2024-11-10T10:03:30.206169Z","iopub.status.idle":"2024-11-10T10:03:30.675439Z","shell.execute_reply.started":"2024-11-10T10:03:30.206133Z","shell.execute_reply":"2024-11-10T10:03:30.674509Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Set the number of epochs\nepochs = 5  # Adjust as needed\n\n# Import AdamW from torch.optim\nfrom torch.optim import AdamW\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n\n# Total number of training steps\ntotal_steps = len(train_loader) * epochs\n\n# Set up the scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:03:33.770473Z","iopub.execute_input":"2024-11-10T10:03:33.770847Z","iopub.status.idle":"2024-11-10T10:03:33.778781Z","shell.execute_reply.started":"2024-11-10T10:03:33.770813Z","shell.execute_reply":"2024-11-10T10:03:33.777775Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Set seed for reproducibility\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:03:35.821002Z","iopub.execute_input":"2024-11-10T10:03:35.821698Z","iopub.status.idle":"2024-11-10T10:03:35.828101Z","shell.execute_reply.started":"2024-11-10T10:03:35.821657Z","shell.execute_reply":"2024-11-10T10:03:35.826511Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:03:37.594053Z","iopub.execute_input":"2024-11-10T10:03:37.594969Z","iopub.status.idle":"2024-11-10T10:03:37.599234Z","shell.execute_reply.started":"2024-11-10T10:03:37.594926Z","shell.execute_reply":"2024-11-10T10:03:37.598316Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Store training statistics\ntraining_stats = []\n\n# Measure the total training time\ntotal_t0 = time.time()\n\nfor epoch_i in range(0, epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    \n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    \n    for step, batch in enumerate(train_loader):\n        if step % 40 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n        \n        # Unpack the inputs from the dataloader\n        b_input_ids = batch[0].to(device)\n        b_attention_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Clear any previously calculated gradients\n        model.zero_grad()\n        \n        # Forward pass\n        outputs = model(b_input_ids,\n                        attention_mask=b_attention_mask,\n                        labels=b_labels)\n        \n        # Take the mean of the loss values (for multi-GPU)\n        loss = outputs.loss.mean()\n        logits = outputs.logits\n        \n        total_train_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update parameters and scheduler\n        optimizer.step()\n        scheduler.step()\n    \n    # Calculate the average loss over all batches\n    avg_train_loss = total_train_loss / len(train_loader)\n    \n    training_time = format_time(time.time() - t0)\n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(training_time))\n    \n    # ========================================\n    #               Validation\n    # ========================================\n    print(\"\")\n    print(\"Running Validation...\")\n    \n    t0 = time.time()\n    model.eval()\n    \n    total_eval_loss = 0\n    total_eval_accuracy = 0\n    \n    for batch in val_loader:\n        b_input_ids = batch[0].to(device)\n        b_attention_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():\n            outputs = model(b_input_ids,\n                            attention_mask=b_attention_mask,\n                            labels=b_labels)\n            \n            # Take the mean of the loss values (for multi-GPU)\n            loss = outputs.loss.mean()\n            logits = outputs.logits\n            \n        total_eval_loss += loss.item()\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate accuracy\n        preds = np.argmax(logits, axis=1)\n        total_eval_accuracy += np.sum(preds == label_ids)\n    \n    avg_val_accuracy = total_eval_accuracy / len(val_dataset)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    \n    avg_val_loss = total_eval_loss / len(val_loader)\n    validation_time = format_time(time.time() - t0)\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    \n    # Record statistics\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n    \nprint(\"\")\nprint(\"Training complete!\")\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T10:03:39.323585Z","iopub.execute_input":"2024-11-10T10:03:39.324600Z","iopub.status.idle":"2024-11-10T17:35:32.113878Z","shell.execute_reply.started":"2024-11-10T10:03:39.324548Z","shell.execute_reply":"2024-11-10T17:35:32.112855Z"}},"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTraining...\n  Batch    40  of  4,047.    Elapsed: 0:00:50.\n  Batch    80  of  4,047.    Elapsed: 0:01:42.\n  Batch   120  of  4,047.    Elapsed: 0:02:34.\n  Batch   160  of  4,047.    Elapsed: 0:03:26.\n  Batch   200  of  4,047.    Elapsed: 0:04:18.\n  Batch   240  of  4,047.    Elapsed: 0:05:10.\n  Batch   280  of  4,047.    Elapsed: 0:06:02.\n  Batch   320  of  4,047.    Elapsed: 0:06:54.\n  Batch   360  of  4,047.    Elapsed: 0:07:46.\n  Batch   400  of  4,047.    Elapsed: 0:08:38.\n  Batch   440  of  4,047.    Elapsed: 0:09:31.\n  Batch   480  of  4,047.    Elapsed: 0:10:23.\n  Batch   520  of  4,047.    Elapsed: 0:11:14.\n  Batch   560  of  4,047.    Elapsed: 0:12:06.\n  Batch   600  of  4,047.    Elapsed: 0:12:58.\n  Batch   640  of  4,047.    Elapsed: 0:13:50.\n  Batch   680  of  4,047.    Elapsed: 0:14:42.\n  Batch   720  of  4,047.    Elapsed: 0:15:34.\n  Batch   760  of  4,047.    Elapsed: 0:16:26.\n  Batch   800  of  4,047.    Elapsed: 0:17:18.\n  Batch   840  of  4,047.    Elapsed: 0:18:10.\n  Batch   880  of  4,047.    Elapsed: 0:19:02.\n  Batch   920  of  4,047.    Elapsed: 0:19:54.\n  Batch   960  of  4,047.    Elapsed: 0:20:46.\n  Batch 1,000  of  4,047.    Elapsed: 0:21:38.\n  Batch 1,040  of  4,047.    Elapsed: 0:22:30.\n  Batch 1,080  of  4,047.    Elapsed: 0:23:22.\n  Batch 1,120  of  4,047.    Elapsed: 0:24:14.\n  Batch 1,160  of  4,047.    Elapsed: 0:25:05.\n  Batch 1,200  of  4,047.    Elapsed: 0:25:57.\n  Batch 1,240  of  4,047.    Elapsed: 0:26:49.\n  Batch 1,280  of  4,047.    Elapsed: 0:27:41.\n  Batch 1,320  of  4,047.    Elapsed: 0:28:33.\n  Batch 1,360  of  4,047.    Elapsed: 0:29:25.\n  Batch 1,400  of  4,047.    Elapsed: 0:30:17.\n  Batch 1,440  of  4,047.    Elapsed: 0:31:09.\n  Batch 1,480  of  4,047.    Elapsed: 0:32:01.\n  Batch 1,520  of  4,047.    Elapsed: 0:32:53.\n  Batch 1,560  of  4,047.    Elapsed: 0:33:45.\n  Batch 1,600  of  4,047.    Elapsed: 0:34:37.\n  Batch 1,640  of  4,047.    Elapsed: 0:35:29.\n  Batch 1,680  of  4,047.    Elapsed: 0:36:21.\n  Batch 1,720  of  4,047.    Elapsed: 0:37:13.\n  Batch 1,760  of  4,047.    Elapsed: 0:38:05.\n  Batch 1,800  of  4,047.    Elapsed: 0:38:57.\n  Batch 1,840  of  4,047.    Elapsed: 0:39:49.\n  Batch 1,880  of  4,047.    Elapsed: 0:40:40.\n  Batch 1,920  of  4,047.    Elapsed: 0:41:33.\n  Batch 1,960  of  4,047.    Elapsed: 0:42:24.\n  Batch 2,000  of  4,047.    Elapsed: 0:43:16.\n  Batch 2,040  of  4,047.    Elapsed: 0:44:08.\n  Batch 2,080  of  4,047.    Elapsed: 0:45:00.\n  Batch 2,120  of  4,047.    Elapsed: 0:45:52.\n  Batch 2,160  of  4,047.    Elapsed: 0:46:44.\n  Batch 2,200  of  4,047.    Elapsed: 0:47:36.\n  Batch 2,240  of  4,047.    Elapsed: 0:48:28.\n  Batch 2,280  of  4,047.    Elapsed: 0:49:20.\n  Batch 2,320  of  4,047.    Elapsed: 0:50:11.\n  Batch 2,360  of  4,047.    Elapsed: 0:51:03.\n  Batch 2,400  of  4,047.    Elapsed: 0:51:55.\n  Batch 2,440  of  4,047.    Elapsed: 0:52:47.\n  Batch 2,480  of  4,047.    Elapsed: 0:53:39.\n  Batch 2,520  of  4,047.    Elapsed: 0:54:31.\n  Batch 2,560  of  4,047.    Elapsed: 0:55:23.\n  Batch 2,600  of  4,047.    Elapsed: 0:56:14.\n  Batch 2,640  of  4,047.    Elapsed: 0:57:06.\n  Batch 2,680  of  4,047.    Elapsed: 0:57:58.\n  Batch 2,720  of  4,047.    Elapsed: 0:58:50.\n  Batch 2,760  of  4,047.    Elapsed: 0:59:42.\n  Batch 2,800  of  4,047.    Elapsed: 1:00:33.\n  Batch 2,840  of  4,047.    Elapsed: 1:01:25.\n  Batch 2,880  of  4,047.    Elapsed: 1:02:17.\n  Batch 2,920  of  4,047.    Elapsed: 1:03:09.\n  Batch 2,960  of  4,047.    Elapsed: 1:04:01.\n  Batch 3,000  of  4,047.    Elapsed: 1:04:53.\n  Batch 3,040  of  4,047.    Elapsed: 1:05:44.\n  Batch 3,080  of  4,047.    Elapsed: 1:06:36.\n  Batch 3,120  of  4,047.    Elapsed: 1:07:28.\n  Batch 3,160  of  4,047.    Elapsed: 1:08:20.\n  Batch 3,200  of  4,047.    Elapsed: 1:09:12.\n  Batch 3,240  of  4,047.    Elapsed: 1:10:04.\n  Batch 3,280  of  4,047.    Elapsed: 1:10:55.\n  Batch 3,320  of  4,047.    Elapsed: 1:11:47.\n  Batch 3,360  of  4,047.    Elapsed: 1:12:39.\n  Batch 3,400  of  4,047.    Elapsed: 1:13:31.\n  Batch 3,440  of  4,047.    Elapsed: 1:14:23.\n  Batch 3,480  of  4,047.    Elapsed: 1:15:14.\n  Batch 3,520  of  4,047.    Elapsed: 1:16:06.\n  Batch 3,560  of  4,047.    Elapsed: 1:16:58.\n  Batch 3,600  of  4,047.    Elapsed: 1:17:50.\n  Batch 3,640  of  4,047.    Elapsed: 1:18:42.\n  Batch 3,680  of  4,047.    Elapsed: 1:19:34.\n  Batch 3,720  of  4,047.    Elapsed: 1:20:25.\n  Batch 3,760  of  4,047.    Elapsed: 1:21:17.\n  Batch 3,800  of  4,047.    Elapsed: 1:22:09.\n  Batch 3,840  of  4,047.    Elapsed: 1:23:01.\n  Batch 3,880  of  4,047.    Elapsed: 1:23:52.\n  Batch 3,920  of  4,047.    Elapsed: 1:24:44.\n  Batch 3,960  of  4,047.    Elapsed: 1:25:36.\n  Batch 4,000  of  4,047.    Elapsed: 1:26:28.\n  Batch 4,040  of  4,047.    Elapsed: 1:27:20.\n\n  Average training loss: 0.11\n  Training epoch took: 1:27:29\n\nRunning Validation...\n  Accuracy: 0.98\n  Validation Loss: 0.10\n  Validation took: 0:02:59\n\n======== Epoch 2 / 5 ========\nTraining...\n  Batch    40  of  4,047.    Elapsed: 0:00:51.\n  Batch    80  of  4,047.    Elapsed: 0:01:43.\n  Batch   120  of  4,047.    Elapsed: 0:02:35.\n  Batch   160  of  4,047.    Elapsed: 0:03:27.\n  Batch   200  of  4,047.    Elapsed: 0:04:18.\n  Batch   240  of  4,047.    Elapsed: 0:05:10.\n  Batch   280  of  4,047.    Elapsed: 0:06:02.\n  Batch   320  of  4,047.    Elapsed: 0:06:54.\n  Batch   360  of  4,047.    Elapsed: 0:07:46.\n  Batch   400  of  4,047.    Elapsed: 0:08:38.\n  Batch   440  of  4,047.    Elapsed: 0:09:29.\n  Batch   480  of  4,047.    Elapsed: 0:10:21.\n  Batch   520  of  4,047.    Elapsed: 0:11:13.\n  Batch   560  of  4,047.    Elapsed: 0:12:04.\n  Batch   600  of  4,047.    Elapsed: 0:12:56.\n  Batch   640  of  4,047.    Elapsed: 0:13:48.\n  Batch   680  of  4,047.    Elapsed: 0:14:40.\n  Batch   720  of  4,047.    Elapsed: 0:15:32.\n  Batch   760  of  4,047.    Elapsed: 0:16:23.\n  Batch   800  of  4,047.    Elapsed: 0:17:16.\n  Batch   840  of  4,047.    Elapsed: 0:18:07.\n  Batch   880  of  4,047.    Elapsed: 0:18:59.\n  Batch   920  of  4,047.    Elapsed: 0:19:51.\n  Batch   960  of  4,047.    Elapsed: 0:20:43.\n  Batch 1,000  of  4,047.    Elapsed: 0:21:35.\n  Batch 1,040  of  4,047.    Elapsed: 0:22:26.\n  Batch 1,080  of  4,047.    Elapsed: 0:23:18.\n  Batch 1,120  of  4,047.    Elapsed: 0:24:10.\n  Batch 1,160  of  4,047.    Elapsed: 0:25:01.\n  Batch 1,200  of  4,047.    Elapsed: 0:25:53.\n  Batch 1,240  of  4,047.    Elapsed: 0:26:45.\n  Batch 1,280  of  4,047.    Elapsed: 0:27:37.\n  Batch 1,320  of  4,047.    Elapsed: 0:28:28.\n  Batch 1,360  of  4,047.    Elapsed: 0:29:20.\n  Batch 1,400  of  4,047.    Elapsed: 0:30:12.\n  Batch 1,440  of  4,047.    Elapsed: 0:31:04.\n  Batch 1,480  of  4,047.    Elapsed: 0:31:56.\n  Batch 1,520  of  4,047.    Elapsed: 0:32:47.\n  Batch 1,560  of  4,047.    Elapsed: 0:33:39.\n  Batch 1,600  of  4,047.    Elapsed: 0:34:31.\n  Batch 1,640  of  4,047.    Elapsed: 0:35:23.\n  Batch 1,680  of  4,047.    Elapsed: 0:36:15.\n  Batch 1,720  of  4,047.    Elapsed: 0:37:07.\n  Batch 1,760  of  4,047.    Elapsed: 0:37:58.\n  Batch 1,800  of  4,047.    Elapsed: 0:38:50.\n  Batch 1,840  of  4,047.    Elapsed: 0:39:42.\n  Batch 1,880  of  4,047.    Elapsed: 0:40:34.\n  Batch 1,920  of  4,047.    Elapsed: 0:41:26.\n  Batch 1,960  of  4,047.    Elapsed: 0:42:17.\n  Batch 2,000  of  4,047.    Elapsed: 0:43:09.\n  Batch 2,040  of  4,047.    Elapsed: 0:44:01.\n  Batch 2,080  of  4,047.    Elapsed: 0:44:53.\n  Batch 2,120  of  4,047.    Elapsed: 0:45:45.\n  Batch 2,160  of  4,047.    Elapsed: 0:46:37.\n  Batch 2,200  of  4,047.    Elapsed: 0:47:28.\n  Batch 2,240  of  4,047.    Elapsed: 0:48:20.\n  Batch 2,280  of  4,047.    Elapsed: 0:49:12.\n  Batch 2,320  of  4,047.    Elapsed: 0:50:04.\n  Batch 2,360  of  4,047.    Elapsed: 0:50:56.\n  Batch 2,400  of  4,047.    Elapsed: 0:51:47.\n  Batch 2,440  of  4,047.    Elapsed: 0:52:39.\n  Batch 2,480  of  4,047.    Elapsed: 0:53:31.\n  Batch 2,520  of  4,047.    Elapsed: 0:54:23.\n  Batch 2,560  of  4,047.    Elapsed: 0:55:15.\n  Batch 2,600  of  4,047.    Elapsed: 0:56:06.\n  Batch 2,640  of  4,047.    Elapsed: 0:56:58.\n  Batch 2,680  of  4,047.    Elapsed: 0:57:50.\n  Batch 2,720  of  4,047.    Elapsed: 0:58:42.\n  Batch 2,760  of  4,047.    Elapsed: 0:59:34.\n  Batch 2,800  of  4,047.    Elapsed: 1:00:25.\n  Batch 2,840  of  4,047.    Elapsed: 1:01:17.\n  Batch 2,880  of  4,047.    Elapsed: 1:02:09.\n  Batch 2,920  of  4,047.    Elapsed: 1:03:01.\n  Batch 2,960  of  4,047.    Elapsed: 1:03:53.\n  Batch 3,000  of  4,047.    Elapsed: 1:04:45.\n  Batch 3,040  of  4,047.    Elapsed: 1:05:37.\n  Batch 3,080  of  4,047.    Elapsed: 1:06:28.\n  Batch 3,120  of  4,047.    Elapsed: 1:07:20.\n  Batch 3,160  of  4,047.    Elapsed: 1:08:12.\n  Batch 3,200  of  4,047.    Elapsed: 1:09:04.\n  Batch 3,240  of  4,047.    Elapsed: 1:09:56.\n  Batch 3,280  of  4,047.    Elapsed: 1:10:47.\n  Batch 3,320  of  4,047.    Elapsed: 1:11:40.\n  Batch 3,360  of  4,047.    Elapsed: 1:12:32.\n  Batch 3,400  of  4,047.    Elapsed: 1:13:24.\n  Batch 3,440  of  4,047.    Elapsed: 1:14:15.\n  Batch 3,480  of  4,047.    Elapsed: 1:15:07.\n  Batch 3,520  of  4,047.    Elapsed: 1:15:59.\n  Batch 3,560  of  4,047.    Elapsed: 1:16:52.\n  Batch 3,600  of  4,047.    Elapsed: 1:17:43.\n  Batch 3,640  of  4,047.    Elapsed: 1:18:35.\n  Batch 3,680  of  4,047.    Elapsed: 1:19:27.\n  Batch 3,720  of  4,047.    Elapsed: 1:20:19.\n  Batch 3,760  of  4,047.    Elapsed: 1:21:10.\n  Batch 3,800  of  4,047.    Elapsed: 1:22:02.\n  Batch 3,840  of  4,047.    Elapsed: 1:22:54.\n  Batch 3,880  of  4,047.    Elapsed: 1:23:46.\n  Batch 3,920  of  4,047.    Elapsed: 1:24:38.\n  Batch 3,960  of  4,047.    Elapsed: 1:25:29.\n  Batch 4,000  of  4,047.    Elapsed: 1:26:21.\n  Batch 4,040  of  4,047.    Elapsed: 1:27:13.\n\n  Average training loss: 0.08\n  Training epoch took: 1:27:22\n\nRunning Validation...\n  Accuracy: 0.98\n  Validation Loss: 0.09\n  Validation took: 0:02:59\n\n======== Epoch 3 / 5 ========\nTraining...\n  Batch    40  of  4,047.    Elapsed: 0:00:51.\n  Batch    80  of  4,047.    Elapsed: 0:01:43.\n  Batch   120  of  4,047.    Elapsed: 0:02:34.\n  Batch   160  of  4,047.    Elapsed: 0:03:26.\n  Batch   200  of  4,047.    Elapsed: 0:04:18.\n  Batch   240  of  4,047.    Elapsed: 0:05:10.\n  Batch   280  of  4,047.    Elapsed: 0:06:02.\n  Batch   320  of  4,047.    Elapsed: 0:06:54.\n  Batch   360  of  4,047.    Elapsed: 0:07:46.\n  Batch   400  of  4,047.    Elapsed: 0:08:37.\n  Batch   440  of  4,047.    Elapsed: 0:09:29.\n  Batch   480  of  4,047.    Elapsed: 0:10:21.\n  Batch   520  of  4,047.    Elapsed: 0:11:13.\n  Batch   560  of  4,047.    Elapsed: 0:12:05.\n  Batch   600  of  4,047.    Elapsed: 0:12:57.\n  Batch   640  of  4,047.    Elapsed: 0:13:49.\n  Batch   680  of  4,047.    Elapsed: 0:14:40.\n  Batch   720  of  4,047.    Elapsed: 0:15:32.\n  Batch   760  of  4,047.    Elapsed: 0:16:24.\n  Batch   800  of  4,047.    Elapsed: 0:17:16.\n  Batch   840  of  4,047.    Elapsed: 0:18:08.\n  Batch   880  of  4,047.    Elapsed: 0:18:59.\n  Batch   920  of  4,047.    Elapsed: 0:19:51.\n  Batch   960  of  4,047.    Elapsed: 0:20:43.\n  Batch 1,000  of  4,047.    Elapsed: 0:21:35.\n  Batch 1,040  of  4,047.    Elapsed: 0:22:27.\n  Batch 1,080  of  4,047.    Elapsed: 0:23:19.\n  Batch 1,120  of  4,047.    Elapsed: 0:24:10.\n  Batch 1,160  of  4,047.    Elapsed: 0:25:02.\n  Batch 1,200  of  4,047.    Elapsed: 0:25:54.\n  Batch 1,240  of  4,047.    Elapsed: 0:26:46.\n  Batch 1,280  of  4,047.    Elapsed: 0:27:38.\n  Batch 1,320  of  4,047.    Elapsed: 0:28:30.\n  Batch 1,360  of  4,047.    Elapsed: 0:29:21.\n  Batch 1,400  of  4,047.    Elapsed: 0:30:13.\n  Batch 1,440  of  4,047.    Elapsed: 0:31:05.\n  Batch 1,480  of  4,047.    Elapsed: 0:31:57.\n  Batch 1,520  of  4,047.    Elapsed: 0:32:49.\n  Batch 1,560  of  4,047.    Elapsed: 0:33:40.\n  Batch 1,600  of  4,047.    Elapsed: 0:34:32.\n  Batch 1,640  of  4,047.    Elapsed: 0:35:24.\n  Batch 1,680  of  4,047.    Elapsed: 0:36:16.\n  Batch 1,720  of  4,047.    Elapsed: 0:37:08.\n  Batch 1,760  of  4,047.    Elapsed: 0:38:00.\n  Batch 1,800  of  4,047.    Elapsed: 0:38:51.\n  Batch 1,840  of  4,047.    Elapsed: 0:39:43.\n  Batch 1,880  of  4,047.    Elapsed: 0:40:35.\n  Batch 1,920  of  4,047.    Elapsed: 0:41:27.\n  Batch 1,960  of  4,047.    Elapsed: 0:42:19.\n  Batch 2,000  of  4,047.    Elapsed: 0:43:10.\n  Batch 2,040  of  4,047.    Elapsed: 0:44:02.\n  Batch 2,080  of  4,047.    Elapsed: 0:44:54.\n  Batch 2,120  of  4,047.    Elapsed: 0:45:46.\n  Batch 2,160  of  4,047.    Elapsed: 0:46:38.\n  Batch 2,200  of  4,047.    Elapsed: 0:47:29.\n  Batch 2,240  of  4,047.    Elapsed: 0:48:21.\n  Batch 2,280  of  4,047.    Elapsed: 0:49:13.\n  Batch 2,320  of  4,047.    Elapsed: 0:50:05.\n  Batch 2,360  of  4,047.    Elapsed: 0:50:57.\n  Batch 2,400  of  4,047.    Elapsed: 0:51:48.\n  Batch 2,440  of  4,047.    Elapsed: 0:52:40.\n  Batch 2,480  of  4,047.    Elapsed: 0:53:32.\n  Batch 2,520  of  4,047.    Elapsed: 0:54:24.\n  Batch 2,560  of  4,047.    Elapsed: 0:55:16.\n  Batch 2,600  of  4,047.    Elapsed: 0:56:08.\n  Batch 2,640  of  4,047.    Elapsed: 0:56:59.\n  Batch 2,680  of  4,047.    Elapsed: 0:57:51.\n  Batch 2,720  of  4,047.    Elapsed: 0:58:43.\n  Batch 2,760  of  4,047.    Elapsed: 0:59:35.\n  Batch 2,800  of  4,047.    Elapsed: 1:00:27.\n  Batch 2,840  of  4,047.    Elapsed: 1:01:18.\n  Batch 2,880  of  4,047.    Elapsed: 1:02:10.\n  Batch 2,920  of  4,047.    Elapsed: 1:03:02.\n  Batch 2,960  of  4,047.    Elapsed: 1:03:54.\n  Batch 3,000  of  4,047.    Elapsed: 1:04:46.\n  Batch 3,040  of  4,047.    Elapsed: 1:05:38.\n  Batch 3,080  of  4,047.    Elapsed: 1:06:29.\n  Batch 3,120  of  4,047.    Elapsed: 1:07:21.\n  Batch 3,160  of  4,047.    Elapsed: 1:08:13.\n  Batch 3,200  of  4,047.    Elapsed: 1:09:05.\n  Batch 3,240  of  4,047.    Elapsed: 1:09:57.\n  Batch 3,280  of  4,047.    Elapsed: 1:10:49.\n  Batch 3,320  of  4,047.    Elapsed: 1:11:41.\n  Batch 3,360  of  4,047.    Elapsed: 1:12:32.\n  Batch 3,400  of  4,047.    Elapsed: 1:13:24.\n  Batch 3,440  of  4,047.    Elapsed: 1:14:16.\n  Batch 3,480  of  4,047.    Elapsed: 1:15:08.\n  Batch 3,520  of  4,047.    Elapsed: 1:16:00.\n  Batch 3,560  of  4,047.    Elapsed: 1:16:51.\n  Batch 3,600  of  4,047.    Elapsed: 1:17:43.\n  Batch 3,640  of  4,047.    Elapsed: 1:18:35.\n  Batch 3,680  of  4,047.    Elapsed: 1:19:27.\n  Batch 3,720  of  4,047.    Elapsed: 1:20:19.\n  Batch 3,760  of  4,047.    Elapsed: 1:21:10.\n  Batch 3,800  of  4,047.    Elapsed: 1:22:02.\n  Batch 3,840  of  4,047.    Elapsed: 1:22:54.\n  Batch 3,880  of  4,047.    Elapsed: 1:23:46.\n  Batch 3,920  of  4,047.    Elapsed: 1:24:38.\n  Batch 3,960  of  4,047.    Elapsed: 1:25:30.\n  Batch 4,000  of  4,047.    Elapsed: 1:26:21.\n  Batch 4,040  of  4,047.    Elapsed: 1:27:13.\n\n  Average training loss: 0.07\n  Training epoch took: 1:27:22\n\nRunning Validation...\n  Accuracy: 0.99\n  Validation Loss: 0.05\n  Validation took: 0:02:59\n\n======== Epoch 4 / 5 ========\nTraining...\n  Batch    40  of  4,047.    Elapsed: 0:00:51.\n  Batch    80  of  4,047.    Elapsed: 0:01:43.\n  Batch   120  of  4,047.    Elapsed: 0:02:35.\n  Batch   160  of  4,047.    Elapsed: 0:03:27.\n  Batch   200  of  4,047.    Elapsed: 0:04:18.\n  Batch   240  of  4,047.    Elapsed: 0:05:10.\n  Batch   280  of  4,047.    Elapsed: 0:06:02.\n  Batch   320  of  4,047.    Elapsed: 0:06:54.\n  Batch   360  of  4,047.    Elapsed: 0:07:46.\n  Batch   400  of  4,047.    Elapsed: 0:08:38.\n  Batch   440  of  4,047.    Elapsed: 0:09:30.\n  Batch   480  of  4,047.    Elapsed: 0:10:22.\n  Batch   520  of  4,047.    Elapsed: 0:11:13.\n  Batch   560  of  4,047.    Elapsed: 0:12:05.\n  Batch   600  of  4,047.    Elapsed: 0:12:57.\n  Batch   640  of  4,047.    Elapsed: 0:13:49.\n  Batch   680  of  4,047.    Elapsed: 0:14:41.\n  Batch   720  of  4,047.    Elapsed: 0:15:33.\n  Batch   760  of  4,047.    Elapsed: 0:16:25.\n  Batch   800  of  4,047.    Elapsed: 0:17:16.\n  Batch   840  of  4,047.    Elapsed: 0:18:08.\n  Batch   880  of  4,047.    Elapsed: 0:19:00.\n  Batch   920  of  4,047.    Elapsed: 0:19:52.\n  Batch   960  of  4,047.    Elapsed: 0:20:44.\n  Batch 1,000  of  4,047.    Elapsed: 0:21:36.\n  Batch 1,040  of  4,047.    Elapsed: 0:22:28.\n  Batch 1,080  of  4,047.    Elapsed: 0:23:20.\n  Batch 1,120  of  4,047.    Elapsed: 0:24:11.\n  Batch 1,160  of  4,047.    Elapsed: 0:25:03.\n  Batch 1,200  of  4,047.    Elapsed: 0:25:55.\n  Batch 1,240  of  4,047.    Elapsed: 0:26:47.\n  Batch 1,280  of  4,047.    Elapsed: 0:27:39.\n  Batch 1,320  of  4,047.    Elapsed: 0:28:30.\n  Batch 1,360  of  4,047.    Elapsed: 0:29:22.\n  Batch 1,400  of  4,047.    Elapsed: 0:30:14.\n  Batch 1,440  of  4,047.    Elapsed: 0:31:06.\n  Batch 1,480  of  4,047.    Elapsed: 0:31:58.\n  Batch 1,520  of  4,047.    Elapsed: 0:32:50.\n  Batch 1,560  of  4,047.    Elapsed: 0:33:42.\n  Batch 1,600  of  4,047.    Elapsed: 0:34:34.\n  Batch 1,640  of  4,047.    Elapsed: 0:35:26.\n  Batch 1,680  of  4,047.    Elapsed: 0:36:17.\n  Batch 1,720  of  4,047.    Elapsed: 0:37:09.\n  Batch 1,760  of  4,047.    Elapsed: 0:38:01.\n  Batch 1,800  of  4,047.    Elapsed: 0:38:53.\n  Batch 1,840  of  4,047.    Elapsed: 0:39:45.\n  Batch 1,880  of  4,047.    Elapsed: 0:40:37.\n  Batch 1,920  of  4,047.    Elapsed: 0:41:29.\n  Batch 1,960  of  4,047.    Elapsed: 0:42:20.\n  Batch 2,000  of  4,047.    Elapsed: 0:43:13.\n  Batch 2,040  of  4,047.    Elapsed: 0:44:04.\n  Batch 2,080  of  4,047.    Elapsed: 0:44:56.\n  Batch 2,120  of  4,047.    Elapsed: 0:45:48.\n  Batch 2,160  of  4,047.    Elapsed: 0:46:40.\n  Batch 2,200  of  4,047.    Elapsed: 0:47:32.\n  Batch 2,240  of  4,047.    Elapsed: 0:48:24.\n  Batch 2,280  of  4,047.    Elapsed: 0:49:16.\n  Batch 2,320  of  4,047.    Elapsed: 0:50:07.\n  Batch 2,360  of  4,047.    Elapsed: 0:50:59.\n  Batch 2,400  of  4,047.    Elapsed: 0:51:51.\n  Batch 2,440  of  4,047.    Elapsed: 0:52:43.\n  Batch 2,480  of  4,047.    Elapsed: 0:53:34.\n  Batch 2,520  of  4,047.    Elapsed: 0:54:26.\n  Batch 2,560  of  4,047.    Elapsed: 0:55:18.\n  Batch 2,600  of  4,047.    Elapsed: 0:56:10.\n  Batch 2,640  of  4,047.    Elapsed: 0:57:02.\n  Batch 2,680  of  4,047.    Elapsed: 0:57:54.\n  Batch 2,720  of  4,047.    Elapsed: 0:58:46.\n  Batch 2,760  of  4,047.    Elapsed: 0:59:37.\n  Batch 2,800  of  4,047.    Elapsed: 1:00:29.\n  Batch 2,840  of  4,047.    Elapsed: 1:01:21.\n  Batch 2,880  of  4,047.    Elapsed: 1:02:13.\n  Batch 2,920  of  4,047.    Elapsed: 1:03:05.\n  Batch 2,960  of  4,047.    Elapsed: 1:03:57.\n  Batch 3,000  of  4,047.    Elapsed: 1:04:48.\n  Batch 3,040  of  4,047.    Elapsed: 1:05:40.\n  Batch 3,080  of  4,047.    Elapsed: 1:06:32.\n  Batch 3,120  of  4,047.    Elapsed: 1:07:24.\n  Batch 3,160  of  4,047.    Elapsed: 1:08:16.\n  Batch 3,200  of  4,047.    Elapsed: 1:09:07.\n  Batch 3,240  of  4,047.    Elapsed: 1:09:59.\n  Batch 3,280  of  4,047.    Elapsed: 1:10:51.\n  Batch 3,320  of  4,047.    Elapsed: 1:11:43.\n  Batch 3,360  of  4,047.    Elapsed: 1:12:35.\n  Batch 3,400  of  4,047.    Elapsed: 1:13:26.\n  Batch 3,440  of  4,047.    Elapsed: 1:14:18.\n  Batch 3,480  of  4,047.    Elapsed: 1:15:10.\n  Batch 3,520  of  4,047.    Elapsed: 1:16:02.\n  Batch 3,560  of  4,047.    Elapsed: 1:16:54.\n  Batch 3,600  of  4,047.    Elapsed: 1:17:46.\n  Batch 3,640  of  4,047.    Elapsed: 1:18:37.\n  Batch 3,680  of  4,047.    Elapsed: 1:19:29.\n  Batch 3,720  of  4,047.    Elapsed: 1:20:21.\n  Batch 3,760  of  4,047.    Elapsed: 1:21:13.\n  Batch 3,800  of  4,047.    Elapsed: 1:22:04.\n  Batch 3,840  of  4,047.    Elapsed: 1:22:56.\n  Batch 3,880  of  4,047.    Elapsed: 1:23:48.\n  Batch 3,920  of  4,047.    Elapsed: 1:24:40.\n  Batch 3,960  of  4,047.    Elapsed: 1:25:32.\n  Batch 4,000  of  4,047.    Elapsed: 1:26:24.\n  Batch 4,040  of  4,047.    Elapsed: 1:27:15.\n\n  Average training loss: 0.05\n  Training epoch took: 1:27:24\n\nRunning Validation...\n  Accuracy: 0.99\n  Validation Loss: 0.06\n  Validation took: 0:02:59\n\n======== Epoch 5 / 5 ========\nTraining...\n  Batch    40  of  4,047.    Elapsed: 0:00:51.\n  Batch    80  of  4,047.    Elapsed: 0:01:43.\n  Batch   120  of  4,047.    Elapsed: 0:02:35.\n  Batch   160  of  4,047.    Elapsed: 0:03:27.\n  Batch   200  of  4,047.    Elapsed: 0:04:18.\n  Batch   240  of  4,047.    Elapsed: 0:05:10.\n  Batch   280  of  4,047.    Elapsed: 0:06:02.\n  Batch   320  of  4,047.    Elapsed: 0:06:54.\n  Batch   360  of  4,047.    Elapsed: 0:07:46.\n  Batch   400  of  4,047.    Elapsed: 0:08:38.\n  Batch   440  of  4,047.    Elapsed: 0:09:29.\n  Batch   480  of  4,047.    Elapsed: 0:10:21.\n  Batch   520  of  4,047.    Elapsed: 0:11:13.\n  Batch   560  of  4,047.    Elapsed: 0:12:05.\n  Batch   600  of  4,047.    Elapsed: 0:12:56.\n  Batch   640  of  4,047.    Elapsed: 0:13:48.\n  Batch   680  of  4,047.    Elapsed: 0:14:40.\n  Batch   720  of  4,047.    Elapsed: 0:15:32.\n  Batch   760  of  4,047.    Elapsed: 0:16:24.\n  Batch   800  of  4,047.    Elapsed: 0:17:16.\n  Batch   840  of  4,047.    Elapsed: 0:18:07.\n  Batch   880  of  4,047.    Elapsed: 0:18:59.\n  Batch   920  of  4,047.    Elapsed: 0:19:51.\n  Batch   960  of  4,047.    Elapsed: 0:20:43.\n  Batch 1,000  of  4,047.    Elapsed: 0:21:35.\n  Batch 1,040  of  4,047.    Elapsed: 0:22:27.\n  Batch 1,080  of  4,047.    Elapsed: 0:23:18.\n  Batch 1,120  of  4,047.    Elapsed: 0:24:10.\n  Batch 1,160  of  4,047.    Elapsed: 0:25:02.\n  Batch 1,200  of  4,047.    Elapsed: 0:25:54.\n  Batch 1,240  of  4,047.    Elapsed: 0:26:46.\n  Batch 1,280  of  4,047.    Elapsed: 0:27:38.\n  Batch 1,320  of  4,047.    Elapsed: 0:28:30.\n  Batch 1,360  of  4,047.    Elapsed: 0:29:22.\n  Batch 1,400  of  4,047.    Elapsed: 0:30:14.\n  Batch 1,440  of  4,047.    Elapsed: 0:31:05.\n  Batch 1,480  of  4,047.    Elapsed: 0:31:57.\n  Batch 1,520  of  4,047.    Elapsed: 0:32:49.\n  Batch 1,560  of  4,047.    Elapsed: 0:33:41.\n  Batch 1,600  of  4,047.    Elapsed: 0:34:33.\n  Batch 1,640  of  4,047.    Elapsed: 0:35:24.\n  Batch 1,680  of  4,047.    Elapsed: 0:36:16.\n  Batch 1,720  of  4,047.    Elapsed: 0:37:08.\n  Batch 1,760  of  4,047.    Elapsed: 0:38:00.\n  Batch 1,800  of  4,047.    Elapsed: 0:38:52.\n  Batch 1,840  of  4,047.    Elapsed: 0:39:43.\n  Batch 1,880  of  4,047.    Elapsed: 0:40:35.\n  Batch 1,920  of  4,047.    Elapsed: 0:41:27.\n  Batch 1,960  of  4,047.    Elapsed: 0:42:19.\n  Batch 2,000  of  4,047.    Elapsed: 0:43:10.\n  Batch 2,040  of  4,047.    Elapsed: 0:44:02.\n  Batch 2,080  of  4,047.    Elapsed: 0:44:54.\n  Batch 2,120  of  4,047.    Elapsed: 0:45:46.\n  Batch 2,160  of  4,047.    Elapsed: 0:46:38.\n  Batch 2,200  of  4,047.    Elapsed: 0:47:30.\n  Batch 2,240  of  4,047.    Elapsed: 0:48:21.\n  Batch 2,280  of  4,047.    Elapsed: 0:49:13.\n  Batch 2,320  of  4,047.    Elapsed: 0:50:05.\n  Batch 2,360  of  4,047.    Elapsed: 0:50:57.\n  Batch 2,400  of  4,047.    Elapsed: 0:51:49.\n  Batch 2,440  of  4,047.    Elapsed: 0:52:40.\n  Batch 2,480  of  4,047.    Elapsed: 0:53:32.\n  Batch 2,520  of  4,047.    Elapsed: 0:54:24.\n  Batch 2,560  of  4,047.    Elapsed: 0:55:15.\n  Batch 2,600  of  4,047.    Elapsed: 0:56:07.\n  Batch 2,640  of  4,047.    Elapsed: 0:56:59.\n  Batch 2,680  of  4,047.    Elapsed: 0:57:51.\n  Batch 2,720  of  4,047.    Elapsed: 0:58:43.\n  Batch 2,760  of  4,047.    Elapsed: 0:59:34.\n  Batch 2,800  of  4,047.    Elapsed: 1:00:26.\n  Batch 2,840  of  4,047.    Elapsed: 1:01:18.\n  Batch 2,880  of  4,047.    Elapsed: 1:02:10.\n  Batch 2,920  of  4,047.    Elapsed: 1:03:02.\n  Batch 2,960  of  4,047.    Elapsed: 1:03:53.\n  Batch 3,000  of  4,047.    Elapsed: 1:04:45.\n  Batch 3,040  of  4,047.    Elapsed: 1:05:37.\n  Batch 3,080  of  4,047.    Elapsed: 1:06:29.\n  Batch 3,120  of  4,047.    Elapsed: 1:07:20.\n  Batch 3,160  of  4,047.    Elapsed: 1:08:12.\n  Batch 3,200  of  4,047.    Elapsed: 1:09:04.\n  Batch 3,240  of  4,047.    Elapsed: 1:09:55.\n  Batch 3,280  of  4,047.    Elapsed: 1:10:47.\n  Batch 3,320  of  4,047.    Elapsed: 1:11:39.\n  Batch 3,360  of  4,047.    Elapsed: 1:12:31.\n  Batch 3,400  of  4,047.    Elapsed: 1:13:22.\n  Batch 3,440  of  4,047.    Elapsed: 1:14:14.\n  Batch 3,480  of  4,047.    Elapsed: 1:15:06.\n  Batch 3,520  of  4,047.    Elapsed: 1:15:58.\n  Batch 3,560  of  4,047.    Elapsed: 1:16:50.\n  Batch 3,600  of  4,047.    Elapsed: 1:17:41.\n  Batch 3,640  of  4,047.    Elapsed: 1:18:33.\n  Batch 3,680  of  4,047.    Elapsed: 1:19:25.\n  Batch 3,720  of  4,047.    Elapsed: 1:20:16.\n  Batch 3,760  of  4,047.    Elapsed: 1:21:08.\n  Batch 3,800  of  4,047.    Elapsed: 1:22:00.\n  Batch 3,840  of  4,047.    Elapsed: 1:22:52.\n  Batch 3,880  of  4,047.    Elapsed: 1:23:44.\n  Batch 3,920  of  4,047.    Elapsed: 1:24:36.\n  Batch 3,960  of  4,047.    Elapsed: 1:25:27.\n  Batch 4,000  of  4,047.    Elapsed: 1:26:19.\n  Batch 4,040  of  4,047.    Elapsed: 1:27:11.\n\n  Average training loss: 0.05\n  Training epoch took: 1:27:20\n\nRunning Validation...\n  Accuracy: 0.99\n  Validation Loss: 0.05\n  Validation took: 0:02:59\n\nTraining complete!\nTotal training took 7:31:53 (h:mm:ss)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \narticle_predictions = {}\narticle_true_labels = {}\n\nfor batch in val_loader:\n    b_input_ids = batch[0].to(device)\n    b_attention_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n    b_article_ids = batch[3].to('cpu').numpy()\n    \n    with torch.no_grad():\n        outputs = model(b_input_ids,\n                        attention_mask=b_attention_mask)\n        \n        logits = outputs.logits\n    \n    logits = logits.detach().cpu().numpy()\n    preds = np.argmax(logits, axis=1)\n    label_ids = b_labels.to('cpu').numpy()\n    \n    for article_id, pred, true_label in zip(b_article_ids, preds, label_ids):\n        article_id = int(article_id)\n        if article_id not in article_predictions:\n            article_predictions[article_id] = []\n            article_true_labels[article_id] = true_label\n        article_predictions[article_id].append(pred)\n\n# Now, aggregate predictions per article\nfinal_predictions = []\nfinal_true_labels = []\n\nfor article_id in article_predictions.keys():\n    preds = article_predictions[article_id]\n    true_label = article_true_labels[article_id]\n    \n    # Majority vote\n    final_pred = max(set(preds), key=preds.count)\n    \n    final_predictions.append(final_pred)\n    final_true_labels.append(true_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T17:43:49.485628Z","iopub.execute_input":"2024-11-10T17:43:49.486343Z","iopub.status.idle":"2024-11-10T17:46:48.563372Z","shell.execute_reply.started":"2024-11-10T17:43:49.486306Z","shell.execute_reply":"2024-11-10T17:46:48.562382Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Classification report\nprint(classification_report(final_true_labels, final_predictions))\n\n# Confusion matrix\nconf_mat = confusion_matrix(final_true_labels, final_predictions)\nprint(\"Confusion Matrix:\")\nprint(conf_mat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T17:47:58.516222Z","iopub.execute_input":"2024-11-10T17:47:58.517100Z","iopub.status.idle":"2024-11-10T17:47:58.542383Z","shell.execute_reply.started":"2024-11-10T17:47:58.517057Z","shell.execute_reply":"2024-11-10T17:47:58.541434Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.74      0.82       130\n           1       0.99      1.00      1.00      4868\n\n    accuracy                           0.99      4998\n   macro avg       0.95      0.87      0.91      4998\nweighted avg       0.99      0.99      0.99      4998\n\nConfusion Matrix:\n[[  96   34]\n [   9 4859]]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import os\nimport torch\n\n# Output directory on Kaggle\noutput_dir = '/kaggle/working/model_save/'\n\n# Create directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# Save model (handling DataParallel)\nif isinstance(model, torch.nn.DataParallel):\n    model_to_save = model.module  # Extract the actual model from DataParallel\nelse:\n    model_to_save = model\n\n# Save model, configuration, and tokenizer\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T17:54:03.090156Z","iopub.execute_input":"2024-11-10T17:54:03.091018Z","iopub.status.idle":"2024-11-10T17:54:04.687650Z","shell.execute_reply.started":"2024-11-10T17:54:03.090978Z","shell.execute_reply":"2024-11-10T17:54:04.686745Z"}},"outputs":[{"name":"stdout","text":"Saving model to /kaggle/working/model_save/\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/model_save/tokenizer_config.json',\n '/kaggle/working/model_save/special_tokens_map.json',\n '/kaggle/working/model_save/vocab.txt',\n '/kaggle/working/model_save/added_tokens.json')"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"!pip install huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T17:56:20.752374Z","iopub.execute_input":"2024-11-10T17:56:20.753083Z","iopub.status.idle":"2024-11-10T17:56:32.209050Z","shell.execute_reply.started":"2024-11-10T17:56:20.753044Z","shell.execute_reply":"2024-11-10T17:56:32.207656Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_huggingface_token' with your actual token\nlogin(token=\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T17:57:27.733750Z","iopub.execute_input":"2024-11-10T17:57:27.734691Z","iopub.status.idle":"2024-11-10T17:57:27.873237Z","shell.execute_reply.started":"2024-11-10T17:57:27.734643Z","shell.execute_reply":"2024-11-10T17:57:27.872345Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Repository details\nmodel_name = \"bert-base-multilingual-cased-BanFakeFineTuned\"  # Name for your model\norganization = \"\"  # Replace with your organization name or leave empty if uploading to your personal account\nrepo_id = f\"{organization}/{model_name}\" if organization else model_name\n\n# Create a new repository on Hugging Face\napi = HfApi()\napi.create_repo(repo_id=repo_id, exist_ok=True)\n\n# Push the model files\nmodel_to_save.push_to_hub(repo_id, use_auth_token=True)\ntokenizer.push_to_hub(repo_id, use_auth_token=True)\n\nprint(f\"Model uploaded to Hugging Face at https://huggingface.co/{repo_id}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T18:00:24.722429Z","iopub.execute_input":"2024-11-10T18:00:24.722821Z","iopub.status.idle":"2024-11-10T18:00:47.780735Z","shell.execute_reply.started":"2024-11-10T18:00:24.722787Z","shell.execute_reply":"2024-11-10T18:00:47.779802Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f869ef2d024f4ba3394bfcf591c2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ef4451743c4aac8ed35f36db675fe8"}},"metadata":{}},{"name":"stdout","text":"Model uploaded to Hugging Face at https://huggingface.co/bert-base-multilingual-cased-BanFakeFineTuned\n","output_type":"stream"}],"execution_count":36}]}