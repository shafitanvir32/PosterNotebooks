{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9914171,"sourceType":"datasetVersion","datasetId":6092139},{"sourceId":9914866,"sourceType":"datasetVersion","datasetId":6092678}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndef calculate_metrics(csv_path):\n    # Load the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Initialize counters\n    total_correct_predictions = 0\n    total_predictions = len(df)\n    true_positives = false_positives = true_negatives = false_negatives = 0\n\n    # Grouped accuracies by category\n    knowledge_correct = knowledge_total = 0\n    textual_correct = textual_total = 0\n    common_sense_correct = common_sense_total = 0\n\n    # Accuracy and correct count by year\n    year_accuracies = {}\n\n    # Iterate through each row in the DataFrame\n    for _, row in df.iterrows():\n        label = row['LABEL']\n        prediction = row['predictions']\n        base = row['BASE'].lower()\n        year = row['YEAR']\n\n        # Check correct prediction\n        if label == prediction:\n            total_correct_predictions += 1\n            if label == 0:\n                true_negatives += 1\n            else:\n                true_positives += 1\n        else:\n            if label == 0:\n                false_positives += 1\n            else:\n                false_negatives += 1\n\n        # Update category-specific counters\n        if base == 'knowledge based':\n            knowledge_total += 1\n            if label == prediction:\n                knowledge_correct += 1\n        elif base == 'sentimental':\n            textual_total += 1\n            if label == prediction:\n                textual_correct += 1\n        elif base == 'common sense':\n            common_sense_total += 1\n            if label == prediction:\n                common_sense_correct += 1\n\n        # Update year-specific counters\n        if year not in year_accuracies:\n            year_accuracies[year] = {'correct': 0, 'total': 0}\n        year_accuracies[year]['total'] += 1\n        if label == prediction:\n            year_accuracies[year]['correct'] += 1\n\n    # Calculate metrics\n    fake_accuracy = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n    real_accuracy = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    knowledge_based_accuracy = knowledge_correct / knowledge_total if knowledge_total > 0 else 0\n    textual_based_accuracy = textual_correct / textual_total if textual_total > 0 else 0\n    common_sense_based_accuracy = common_sense_correct / common_sense_total if common_sense_total > 0 else 0\n    overall_accuracy = total_correct_predictions / total_predictions if total_predictions > 0 else 0\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n\n    # Find the year with the highest number of correct predictions\n    best_year = max(year_accuracies.items(), key=lambda x: x[1]['correct'])[0]\n    best_year_correct = year_accuracies[best_year]['correct']\n\n    # Display results\n    results = {\n        'Fake Accuracy': fake_accuracy,\n        'Real Accuracy': real_accuracy,\n        'Knowledge Based Accuracy': knowledge_based_accuracy,\n        'Textual Based Accuracy': textual_based_accuracy,\n        'Common Sense Based Accuracy': common_sense_based_accuracy,\n        'Overall Accuracy': overall_accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'Best Year (Most Correct Predictions)': best_year,\n        'Best Year Correct Predictions': best_year_correct\n    }\n    \n    return results\n\n# Example usage\ncsv_path = '/kaggle/input/wertery5u/predictionsDistil - predictions2.csv.csv'  # Replace with the actual path to your CSV file\nmetrics = calculate_metrics(csv_path)\nfor metric, value in metrics.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.2f}\")\n    else:\n        print(f\"{metric}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T10:49:00.004162Z","iopub.execute_input":"2024-11-15T10:49:00.004689Z","iopub.status.idle":"2024-11-15T10:49:00.083883Z","shell.execute_reply.started":"2024-11-15T10:49:00.004643Z","shell.execute_reply":"2024-11-15T10:49:00.082142Z"}},"outputs":[{"name":"stdout","text":"Fake Accuracy: 0.38\nReal Accuracy: 1.00\nKnowledge Based Accuracy: 0.65\nTextual Based Accuracy: 0.76\nCommon Sense Based Accuracy: 0.68\nOverall Accuracy: 0.69\nPrecision: 0.62\nRecall: 1.00\nBest Year (Most Correct Predictions): 2022.00\nBest Year Correct Predictions: 8\n","output_type":"stream"}],"execution_count":15}]}