{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":203900450,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":7.594014,"end_time":"2024-10-10T11:58:36.355301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T11:58:28.761287","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Useful notebooks:\n\n- Preprocessing : https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags\n- Training (XGB) : https://www.kaggle.com/code/motono0223/js24-train-gbdt-model-with-lags-singlemodel\n  - trained XGB model : https://www.kaggle.com/datasets/motono0223/js24-trained-gbdt-model\n- Training (NN): **this notebook** https://www.kaggle.com/code/voix97/jane-street-rmf-training-nn\n  - trained NN model : https://www.kaggle.com/datasets/voix97/js-xs-nn-trained-model\n- Inference of NN : https://www.kaggle.com/code/voix97/jane-street-rmf-nn-with-pytorch-lightning\n- Inference of NN+XGB:  https://www.kaggle.com/code/voix97/jane-street-rmf-nn-xgb\n- EDA(1) : https://www.kaggle.com/code/motono0223/eda-jane-street-real-time-market-data-forecasting\n- EDA(2) : https://www.kaggle.com/code/motono0223/eda-v2-jane-street-real-time-market-forecasting","metadata":{}},{"cell_type":"markdown","source":"# Training Neural Networks (MLP) with PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport polars as pl\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-11-05T04:58:17.952915Z","iopub.execute_input":"2024-11-05T04:58:17.953615Z","iopub.status.idle":"2024-11-05T04:58:17.962711Z","shell.execute_reply.started":"2024-11-05T04:58:17.953574Z","shell.execute_reply":"2024-11-05T04:58:17.961348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"input_path = './input_df' if os.path.exists('./input_df') else '/kaggle/input/js24-preprocessing-create-lags/training.parquet'\nTRAINING = True\nfeature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\nlabel_name = 'responder_6'\nweight_name = 'weight'\ntrain_name = os.path.join(\"./input_df/\", \"nn_input_df_with_lags.pickle\")\nvalid_name = os.path.join(\"./input_df/\", \"nn_valid_df_with_lags.pickle\")\nif TRAINING and not os.path.exists(train_name):\n    df = pl.scan_parquet(f\"{input_path}/training.parquet\").collect().to_pandas()\n    valid = pl.scan_parquet(f\"{input_path}/validation.parquet\").collect().to_pandas()\n    df = pd.concat([df, valid]).reset_index(drop=True)# A trick to boost LB from 0.0045->0.005\n    with open(train_name, \"wb\") as w:\n        pickle.dump(df, w)\n    with open(valid_name, \"wb\") as w:\n        pickle.dump(valid, w)\nelif TRAINING:\n    with open(train_name, \"rb\") as r:\n        df = pickle.load(r)\n    with open(valid_name, \"rb\") as r:\n        valid = pickle.load(r)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = df[ feature_names ]\ny_train = df[ label_name ]\nw_train = df[ \"weight\" ]\nX_valid = valid[ feature_names ]\ny_valid = valid[ label_name ]\nw_valid = valid[ \"weight\" ]\n\nX_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Configurations","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass custom_args():\n    def __init__(self):\n        self.usegpu = True\n        self.gpuid = 0\n        self.seed = 42\n        self.model = 'nn'\n        self.use_wandb = False\n        self.project = 'js-xs-nn-with-lags'\n        self.dname = \"./input_df/\"\n        self.loader_workers = 4\n        self.bs = 8192\n        self.lr = 1e-3\n        self.weight_decay = 5e-4\n        self.dropouts = [0.1, 0.1]\n        self.n_hidden = [512, 512, 256]\n        self.patience = 25\n        self.max_epochs = 2000\n        self.N_fold = 5\n\n\nmy_args = custom_args()","metadata":{"execution":{"iopub.status.busy":"2024-11-05T04:58:17.964754Z","iopub.execute_input":"2024-11-05T04:58:17.965474Z","iopub.status.idle":"2024-11-05T04:58:17.993759Z","shell.execute_reply.started":"2024-11-05T04:58:17.965427Z","shell.execute_reply":"2024-11-05T04:58:17.992851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PyTorch Data Module Definition","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, accelerator):\n        self.features = torch.FloatTensor(df[feature_names].values).to(accelerator)\n        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        w = self.weights[idx]\n        return x, y, w\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n        super().__init__()\n        self.df = train_df\n        self.batch_size = batch_size\n        self.dates = self.df['date_id'].unique()\n        self.accelerator = accelerator\n        self.train_dataset = None\n        self.valid_df = None\n        if valid_df is not None:\n            self.valid_df = valid_df\n        self.val_dataset = None\n\n    def setup(self, fold=0, N_fold=5, stage=None):\n        # Split dataset\n        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n        self.train_dataset = CustomDataset(df_train, self.accelerator)\n        if self.valid_df is not None:\n            df_valid = self.valid_df\n            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n\n    def train_dataloader(self, n_workers=0):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n\n    def val_dataloader(self, n_workers=0):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T04:58:17.994912Z","iopub.execute_input":"2024-11-05T04:58:17.995267Z","iopub.status.idle":"2024-11-05T04:58:18.776454Z","shell.execute_reply.started":"2024-11-05T04:58:17.995223Z","shell.execute_reply":"2024-11-05T04:58:18.77564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NN Model Definition","metadata":{}},{"cell_type":"code","source":"# Custom R2 metric for validation\ndef r2_val(y_true, y_pred, sample_weight):\n    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n    return r2\n\n\nclass NN(LightningModule):\n    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n        layers = []\n        in_dim = input_dim\n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.append(nn.BatchNorm1d(in_dim))\n            if i > 0:\n                layers.append(nn.SiLU())\n            if i < len(dropouts):\n                layers.append(nn.Dropout(dropouts[i]))\n            layers.append(nn.Linear(in_dim, hidden_dim))\n            # layers.append(nn.ReLU())\n            in_dim = hidden_dim\n        layers.append(nn.Linear(in_dim, 1)) \n        layers.append(nn.Tanh())\n        self.model = nn.Sequential(*layers)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n\n    def forward(self, x):\n        return 5 * self.model(x).squeeze(-1)  \n\n    def training_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        return loss\n\n    def validation_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.validation_step_outputs.append((y_hat, y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n        if self.trainer.sanity_checking:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n        else:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            # r2_val\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n        epoch = self.trainer.current_epoch\n        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T04:58:18.778669Z","iopub.execute_input":"2024-11-05T04:58:18.778953Z","iopub.status.idle":"2024-11-05T04:58:18.796672Z","shell.execute_reply.started":"2024-11-05T04:58:18.778923Z","shell.execute_reply":"2024-11-05T04:58:18.795818Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create PyTorch Data Module","metadata":{}},{"cell_type":"code","source":"\nargs = my_args\n\n# checking device\ndevice = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\naccelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\nloader_device = 'cpu'\n\n\n# Initialize Data Module\n\ndf[feature_names] = df[feature_names].fillna(method = 'ffill').fillna(0)\nvalid[feature_names] = valid[feature_names].fillna(method = 'ffill').fillna(0)\ndata_module = DataModule(df, batch_size=args.bs, valid_df=valid, accelerator=loader_device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Model and Training","metadata":{}},{"cell_type":"code","source":"import gc\ndel df\ngc.collect()\npl.seed_everything(args.seed)\nfor fold in range(args.N_fold):\n    data_module.setup(fold, args.N_fold)\n    # Obtain input dimension\n    input_dim = data_module.train_dataset.features.shape[1]\n    # Initialize Model\n    model = NN(\n        input_dim=input_dim,\n        hidden_dims=args.n_hidden,\n        dropouts=args.dropouts,\n        lr=args.lr,\n        weight_decay=args.weight_decay\n    )\n    # Initialize Logger\n    if args.use_wandb:\n        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n        logger = WandbLogger(experiment=wandb_run)\n    else:\n        logger = None\n    # Initialize Callbacks\n    early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False)\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n    timer = Timer()\n    # Initialize Trainer\n    trainer = Trainer(\n        max_epochs=args.max_epochs,\n        accelerator=accelerator,\n        devices=[args.gpuid] if args.usegpu else None,\n        logger=logger,\n        callbacks=[early_stopping, checkpoint_callback, timer],\n        enable_progress_bar=True\n    )\n    # Start Training\n    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n    # You can find trained best model in your local path\n    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T04:58:18.797992Z","iopub.execute_input":"2024-11-05T04:58:18.798262Z","iopub.status.idle":"2024-11-05T04:58:18.818565Z","shell.execute_reply.started":"2024-11-05T04:58:18.798232Z","shell.execute_reply":"2024-11-05T04:58:18.817654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}