{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9858470,"sourceType":"datasetVersion","datasetId":6050015}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install transformers if not already installed\n!pip install transformers\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport random\nimport time\nimport datetime\nimport os\n\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-10T05:49:08.646785Z","iopub.execute_input":"2024-11-10T05:49:08.647435Z","iopub.status.idle":"2024-11-10T05:49:26.549526Z","shell.execute_reply.started":"2024-11-10T05:49:08.647397Z","shell.execute_reply":"2024-11-10T05:49:26.548503Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/banfakelabelnews/BanFake.csv')\n\n# Check data\nprint(df.head())\nprint('Number of samples:', df.shape[0])\n\n# Check label distribution\nprint(df['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T05:49:26.551078Z","iopub.execute_input":"2024-11-10T05:49:26.551494Z","iopub.status.idle":"2024-11-10T05:49:31.274068Z","shell.execute_reply.started":"2024-11-10T05:49:26.551463Z","shell.execute_reply":"2024-11-10T05:49:31.273052Z"}},"outputs":[{"name":"stdout","text":"   label                                               news\n0      1  হট্টগোল করায় বাকৃবিতে দুইজন বরখাস্ত, ৬ জনকে শো...\n1      1  মালয়েশিয়ায় কর্মী পাঠানোর ব্যবস্থা নেয়ার সুপারি...\n2      1  প্রেমের প্রস্তাবে রাজি না হওয়ায় স্কুলছাত্রীকে ...\n3      1  মেডিয়েশনই মামলাজট নিরসনের পথ : বিচারপতি আহমেদ ...\n4      1  টকশোতে বক্তব্য দিতে গিয়ে জাপা নেতার মৃত্যু মাদ...\nNumber of samples: 49977\nlabel\n1    48678\n0     1299\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n\nprint('Training set size:', train_df.shape[0])\nprint('Validation set size:', val_df.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:22:06.746709Z","iopub.execute_input":"2024-11-10T06:22:06.747583Z","iopub.status.idle":"2024-11-10T06:22:06.783651Z","shell.execute_reply.started":"2024-11-10T06:22:06.747542Z","shell.execute_reply":"2024-11-10T06:22:06.782778Z"}},"outputs":[{"name":"stdout","text":"Training set size: 44979\nValidation set size: 4998\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:23:02.387122Z","iopub.execute_input":"2024-11-10T06:23:02.387511Z","iopub.status.idle":"2024-11-10T06:23:02.397700Z","shell.execute_reply.started":"2024-11-10T06:23:02.387473Z","shell.execute_reply":"2024-11-10T06:23:02.396557Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\n1    43810\n0     1169\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"val_df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:23:47.031467Z","iopub.execute_input":"2024-11-10T06:23:47.032435Z","iopub.status.idle":"2024-11-10T06:23:47.041380Z","shell.execute_reply.started":"2024-11-10T06:23:47.032384Z","shell.execute_reply":"2024-11-10T06:23:47.040252Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"label\n1    4868\n0     130\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\nMAX_LEN = 512  # You can adjust this value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:27:28.072018Z","iopub.execute_input":"2024-11-10T06:27:28.072400Z","iopub.status.idle":"2024-11-10T06:27:30.640033Z","shell.execute_reply.started":"2024-11-10T06:27:28.072364Z","shell.execute_reply":"2024-11-10T06:27:30.639211Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Prepare the training data\ntrain_texts = train_df['news'].tolist()\ntrain_labels = train_df['label'].tolist()\n\n# Prepare the validation data\nval_texts = val_df['news'].tolist()\nval_labels = val_df['label'].tolist()\n\n# Tokenize and encode the training set\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LEN)\n\n# Tokenize and encode the validation set\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:28:07.538313Z","iopub.execute_input":"2024-11-10T06:28:07.539181Z","iopub.status.idle":"2024-11-10T06:31:06.465983Z","shell.execute_reply.started":"2024-11-10T06:28:07.539142Z","shell.execute_reply":"2024-11-10T06:31:06.464943Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        # Replace 'labels' with 'label' if needed\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:31:12.171755Z","iopub.execute_input":"2024-11-10T06:31:12.173079Z","iopub.status.idle":"2024-11-10T06:31:12.179772Z","shell.execute_reply.started":"2024-11-10T06:31:12.173025Z","shell.execute_reply":"2024-11-10T06:31:12.178792Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create the datasets\ntrain_dataset = NewsDataset(train_encodings, train_labels)\nval_dataset = NewsDataset(val_encodings, val_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:31:46.632501Z","iopub.execute_input":"2024-11-10T06:31:46.633205Z","iopub.status.idle":"2024-11-10T06:31:46.637476Z","shell.execute_reply.started":"2024-11-10T06:31:46.633166Z","shell.execute_reply":"2024-11-10T06:31:46.636446Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Set batch size\nbatch_size = 8  # Adjust based on GPU memory\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:32:08.557690Z","iopub.execute_input":"2024-11-10T06:32:08.558583Z","iopub.status.idle":"2024-11-10T06:32:08.563518Z","shell.execute_reply.started":"2024-11-10T06:32:08.558541Z","shell.execute_reply":"2024-11-10T06:32:08.562638Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# Initialize the model\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large')\n\n# Move model to the device\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:32:17.202162Z","iopub.execute_input":"2024-11-10T06:32:17.202675Z","iopub.status.idle":"2024-11-10T06:32:28.743889Z","shell.execute_reply.started":"2024-11-10T06:32:17.202622Z","shell.execute_reply":"2024-11-10T06:32:28.742935Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feff0c476b51473bbc782f4662b9eba3"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Set the number of epochs\nepochs = 2  # You can adjust this value\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n\n# Total number of training steps\ntotal_steps = len(train_loader) * epochs\n\n# Set up the scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:33:02.797257Z","iopub.execute_input":"2024-11-10T06:33:02.798077Z","iopub.status.idle":"2024-11-10T06:33:03.248861Z","shell.execute_reply.started":"2024-11-10T06:33:02.798035Z","shell.execute_reply":"2024-11-10T06:33:03.247910Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Set seed for reproducibility\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:33:14.034482Z","iopub.execute_input":"2024-11-10T06:33:14.035076Z","iopub.status.idle":"2024-11-10T06:33:14.045764Z","shell.execute_reply.started":"2024-11-10T06:33:14.035038Z","shell.execute_reply":"2024-11-10T06:33:14.044827Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:33:23.768760Z","iopub.execute_input":"2024-11-10T06:33:23.769458Z","iopub.status.idle":"2024-11-10T06:33:23.773916Z","shell.execute_reply.started":"2024-11-10T06:33:23.769417Z","shell.execute_reply":"2024-11-10T06:33:23.773011Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Store training statistics\ntraining_stats = []\n\n# Measure the total training time\ntotal_t0 = time.time()\n\nfor epoch_i in range(0, epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    \n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    \n    for step, batch in enumerate(train_loader):\n        if step % 40 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n        \n        # Unpack the inputs from the dataloader\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n        \n        # Clear any previously calculated gradients\n        model.zero_grad()\n        \n        # Forward pass\n        outputs = model(b_input_ids,\n                        attention_mask=b_attention_mask,\n                        labels=b_labels)\n        \n        loss = outputs.loss\n        logits = outputs.logits\n        \n        total_train_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update parameters and scheduler\n        optimizer.step()\n        scheduler.step()\n    \n    # Calculate the average loss over all batches\n    avg_train_loss = total_train_loss / len(train_loader)\n    \n    training_time = format_time(time.time() - t0)\n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(training_time))\n    \n    # ========================================\n    #               Validation\n    # ========================================\n    print(\"\")\n    print(\"Running Validation...\")\n    \n    t0 = time.time()\n    model.eval()\n    \n    total_eval_loss = 0\n    total_eval_accuracy = 0\n    nb_eval_steps = 0\n    \n    for batch in val_loader:\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n        \n        with torch.no_grad():\n            outputs = model(b_input_ids,\n                            attention_mask=b_attention_mask,\n                            labels=b_labels)\n        \n        loss = outputs.loss\n        logits = outputs.logits\n        \n        total_eval_loss += loss.item()\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate accuracy\n        preds = np.argmax(logits, axis=1)\n        total_eval_accuracy += np.sum(preds == label_ids)\n        nb_eval_steps += len(label_ids)\n    \n    avg_val_accuracy = total_eval_accuracy / len(val_dataset)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    \n    avg_val_loss = total_eval_loss / len(val_loader)\n    validation_time = format_time(time.time() - t0)\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    \n    # Record statistics\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n    \nprint(\"\")\nprint(\"Training complete!\")\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T06:34:31.712349Z","iopub.execute_input":"2024-11-10T06:34:31.712840Z","iopub.status.idle":"2024-11-10T15:00:40.911030Z","shell.execute_reply.started":"2024-11-10T06:34:31.712792Z","shell.execute_reply":"2024-11-10T15:00:40.910208Z"}},"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 2 ========\nTraining...\n  Batch    40  of  5,623.    Elapsed: 0:01:44.\n  Batch    80  of  5,623.    Elapsed: 0:03:29.\n  Batch   120  of  5,623.    Elapsed: 0:05:14.\n  Batch   160  of  5,623.    Elapsed: 0:06:59.\n  Batch   200  of  5,623.    Elapsed: 0:08:44.\n  Batch   240  of  5,623.    Elapsed: 0:10:30.\n  Batch   280  of  5,623.    Elapsed: 0:12:15.\n  Batch   320  of  5,623.    Elapsed: 0:14:00.\n  Batch   360  of  5,623.    Elapsed: 0:15:45.\n  Batch   400  of  5,623.    Elapsed: 0:17:29.\n  Batch   440  of  5,623.    Elapsed: 0:19:14.\n  Batch   480  of  5,623.    Elapsed: 0:20:59.\n  Batch   520  of  5,623.    Elapsed: 0:22:44.\n  Batch   560  of  5,623.    Elapsed: 0:24:29.\n  Batch   600  of  5,623.    Elapsed: 0:26:15.\n  Batch   640  of  5,623.    Elapsed: 0:28:00.\n  Batch   680  of  5,623.    Elapsed: 0:29:45.\n  Batch   720  of  5,623.    Elapsed: 0:31:30.\n  Batch   760  of  5,623.    Elapsed: 0:33:15.\n  Batch   800  of  5,623.    Elapsed: 0:35:00.\n  Batch   840  of  5,623.    Elapsed: 0:36:45.\n  Batch   880  of  5,623.    Elapsed: 0:38:30.\n  Batch   920  of  5,623.    Elapsed: 0:40:15.\n  Batch   960  of  5,623.    Elapsed: 0:41:59.\n  Batch 1,000  of  5,623.    Elapsed: 0:43:44.\n  Batch 1,040  of  5,623.    Elapsed: 0:45:29.\n  Batch 1,080  of  5,623.    Elapsed: 0:47:14.\n  Batch 1,120  of  5,623.    Elapsed: 0:48:59.\n  Batch 1,160  of  5,623.    Elapsed: 0:50:44.\n  Batch 1,200  of  5,623.    Elapsed: 0:52:29.\n  Batch 1,240  of  5,623.    Elapsed: 0:54:13.\n  Batch 1,280  of  5,623.    Elapsed: 0:55:58.\n  Batch 1,320  of  5,623.    Elapsed: 0:57:43.\n  Batch 1,360  of  5,623.    Elapsed: 0:59:28.\n  Batch 1,400  of  5,623.    Elapsed: 1:01:13.\n  Batch 1,440  of  5,623.    Elapsed: 1:02:57.\n  Batch 1,480  of  5,623.    Elapsed: 1:04:42.\n  Batch 1,520  of  5,623.    Elapsed: 1:06:27.\n  Batch 1,560  of  5,623.    Elapsed: 1:08:11.\n  Batch 1,600  of  5,623.    Elapsed: 1:09:56.\n  Batch 1,640  of  5,623.    Elapsed: 1:11:41.\n  Batch 1,680  of  5,623.    Elapsed: 1:13:25.\n  Batch 1,720  of  5,623.    Elapsed: 1:15:10.\n  Batch 1,760  of  5,623.    Elapsed: 1:16:55.\n  Batch 1,800  of  5,623.    Elapsed: 1:18:40.\n  Batch 1,840  of  5,623.    Elapsed: 1:20:24.\n  Batch 1,880  of  5,623.    Elapsed: 1:22:09.\n  Batch 1,920  of  5,623.    Elapsed: 1:23:54.\n  Batch 1,960  of  5,623.    Elapsed: 1:25:38.\n  Batch 2,000  of  5,623.    Elapsed: 1:27:23.\n  Batch 2,040  of  5,623.    Elapsed: 1:29:08.\n  Batch 2,080  of  5,623.    Elapsed: 1:30:53.\n  Batch 2,120  of  5,623.    Elapsed: 1:32:37.\n  Batch 2,160  of  5,623.    Elapsed: 1:34:22.\n  Batch 2,200  of  5,623.    Elapsed: 1:36:07.\n  Batch 2,240  of  5,623.    Elapsed: 1:37:52.\n  Batch 2,280  of  5,623.    Elapsed: 1:39:36.\n  Batch 2,320  of  5,623.    Elapsed: 1:41:21.\n  Batch 2,360  of  5,623.    Elapsed: 1:43:06.\n  Batch 2,400  of  5,623.    Elapsed: 1:44:51.\n  Batch 2,440  of  5,623.    Elapsed: 1:46:36.\n  Batch 2,480  of  5,623.    Elapsed: 1:48:20.\n  Batch 2,520  of  5,623.    Elapsed: 1:50:05.\n  Batch 2,560  of  5,623.    Elapsed: 1:51:50.\n  Batch 2,600  of  5,623.    Elapsed: 1:53:35.\n  Batch 2,640  of  5,623.    Elapsed: 1:55:20.\n  Batch 2,680  of  5,623.    Elapsed: 1:57:04.\n  Batch 2,720  of  5,623.    Elapsed: 1:58:49.\n  Batch 2,760  of  5,623.    Elapsed: 2:00:34.\n  Batch 2,800  of  5,623.    Elapsed: 2:02:19.\n  Batch 2,840  of  5,623.    Elapsed: 2:04:04.\n  Batch 2,880  of  5,623.    Elapsed: 2:05:48.\n  Batch 2,920  of  5,623.    Elapsed: 2:07:33.\n  Batch 2,960  of  5,623.    Elapsed: 2:09:18.\n  Batch 3,000  of  5,623.    Elapsed: 2:11:03.\n  Batch 3,040  of  5,623.    Elapsed: 2:12:47.\n  Batch 3,080  of  5,623.    Elapsed: 2:14:32.\n  Batch 3,120  of  5,623.    Elapsed: 2:16:17.\n  Batch 3,160  of  5,623.    Elapsed: 2:18:02.\n  Batch 3,200  of  5,623.    Elapsed: 2:19:47.\n  Batch 3,240  of  5,623.    Elapsed: 2:21:32.\n  Batch 3,280  of  5,623.    Elapsed: 2:23:17.\n  Batch 3,320  of  5,623.    Elapsed: 2:25:02.\n  Batch 3,360  of  5,623.    Elapsed: 2:26:47.\n  Batch 3,400  of  5,623.    Elapsed: 2:28:31.\n  Batch 3,440  of  5,623.    Elapsed: 2:30:16.\n  Batch 3,480  of  5,623.    Elapsed: 2:32:01.\n  Batch 3,520  of  5,623.    Elapsed: 2:33:46.\n  Batch 3,560  of  5,623.    Elapsed: 2:35:30.\n  Batch 3,600  of  5,623.    Elapsed: 2:37:15.\n  Batch 3,640  of  5,623.    Elapsed: 2:39:00.\n  Batch 3,680  of  5,623.    Elapsed: 2:40:45.\n  Batch 3,720  of  5,623.    Elapsed: 2:42:30.\n  Batch 3,760  of  5,623.    Elapsed: 2:44:15.\n  Batch 3,800  of  5,623.    Elapsed: 2:45:59.\n  Batch 3,840  of  5,623.    Elapsed: 2:47:44.\n  Batch 3,880  of  5,623.    Elapsed: 2:49:29.\n  Batch 3,920  of  5,623.    Elapsed: 2:51:14.\n  Batch 3,960  of  5,623.    Elapsed: 2:52:59.\n  Batch 4,000  of  5,623.    Elapsed: 2:54:44.\n  Batch 4,040  of  5,623.    Elapsed: 2:56:28.\n  Batch 4,080  of  5,623.    Elapsed: 2:58:13.\n  Batch 4,120  of  5,623.    Elapsed: 2:59:58.\n  Batch 4,160  of  5,623.    Elapsed: 3:01:43.\n  Batch 4,200  of  5,623.    Elapsed: 3:03:28.\n  Batch 4,240  of  5,623.    Elapsed: 3:05:13.\n  Batch 4,280  of  5,623.    Elapsed: 3:06:57.\n  Batch 4,320  of  5,623.    Elapsed: 3:08:42.\n  Batch 4,360  of  5,623.    Elapsed: 3:10:27.\n  Batch 4,400  of  5,623.    Elapsed: 3:12:12.\n  Batch 4,440  of  5,623.    Elapsed: 3:13:57.\n  Batch 4,480  of  5,623.    Elapsed: 3:15:42.\n  Batch 4,520  of  5,623.    Elapsed: 3:17:27.\n  Batch 4,560  of  5,623.    Elapsed: 3:19:11.\n  Batch 4,600  of  5,623.    Elapsed: 3:20:56.\n  Batch 4,640  of  5,623.    Elapsed: 3:22:41.\n  Batch 4,680  of  5,623.    Elapsed: 3:24:26.\n  Batch 4,720  of  5,623.    Elapsed: 3:26:10.\n  Batch 4,760  of  5,623.    Elapsed: 3:27:55.\n  Batch 4,800  of  5,623.    Elapsed: 3:29:40.\n  Batch 4,840  of  5,623.    Elapsed: 3:31:25.\n  Batch 4,880  of  5,623.    Elapsed: 3:33:10.\n  Batch 4,920  of  5,623.    Elapsed: 3:34:54.\n  Batch 4,960  of  5,623.    Elapsed: 3:36:39.\n  Batch 5,000  of  5,623.    Elapsed: 3:38:24.\n  Batch 5,040  of  5,623.    Elapsed: 3:40:09.\n  Batch 5,080  of  5,623.    Elapsed: 3:41:53.\n  Batch 5,120  of  5,623.    Elapsed: 3:43:38.\n  Batch 5,160  of  5,623.    Elapsed: 3:45:23.\n  Batch 5,200  of  5,623.    Elapsed: 3:47:07.\n  Batch 5,240  of  5,623.    Elapsed: 3:48:52.\n  Batch 5,280  of  5,623.    Elapsed: 3:50:37.\n  Batch 5,320  of  5,623.    Elapsed: 3:52:21.\n  Batch 5,360  of  5,623.    Elapsed: 3:54:06.\n  Batch 5,400  of  5,623.    Elapsed: 3:55:51.\n  Batch 5,440  of  5,623.    Elapsed: 3:57:36.\n  Batch 5,480  of  5,623.    Elapsed: 3:59:20.\n  Batch 5,520  of  5,623.    Elapsed: 4:01:05.\n  Batch 5,560  of  5,623.    Elapsed: 4:02:50.\n  Batch 5,600  of  5,623.    Elapsed: 4:04:34.\n\n  Average training loss: 0.14\n  Training epoch took: 4:05:33\n\nRunning Validation...\n  Accuracy: 0.97\n  Validation Loss: 0.13\n  Validation took: 0:07:34\n\n======== Epoch 2 / 2 ========\nTraining...\n  Batch    40  of  5,623.    Elapsed: 0:01:45.\n  Batch    80  of  5,623.    Elapsed: 0:03:29.\n  Batch   120  of  5,623.    Elapsed: 0:05:14.\n  Batch   160  of  5,623.    Elapsed: 0:06:59.\n  Batch   200  of  5,623.    Elapsed: 0:08:44.\n  Batch   240  of  5,623.    Elapsed: 0:10:28.\n  Batch   280  of  5,623.    Elapsed: 0:12:13.\n  Batch   320  of  5,623.    Elapsed: 0:13:58.\n  Batch   360  of  5,623.    Elapsed: 0:15:43.\n  Batch   400  of  5,623.    Elapsed: 0:17:28.\n  Batch   440  of  5,623.    Elapsed: 0:19:13.\n  Batch   480  of  5,623.    Elapsed: 0:20:57.\n  Batch   520  of  5,623.    Elapsed: 0:22:42.\n  Batch   560  of  5,623.    Elapsed: 0:24:27.\n  Batch   600  of  5,623.    Elapsed: 0:26:11.\n  Batch   640  of  5,623.    Elapsed: 0:27:56.\n  Batch   680  of  5,623.    Elapsed: 0:29:41.\n  Batch   720  of  5,623.    Elapsed: 0:31:26.\n  Batch   760  of  5,623.    Elapsed: 0:33:11.\n  Batch   800  of  5,623.    Elapsed: 0:34:55.\n  Batch   840  of  5,623.    Elapsed: 0:36:40.\n  Batch   880  of  5,623.    Elapsed: 0:38:25.\n  Batch   920  of  5,623.    Elapsed: 0:40:10.\n  Batch   960  of  5,623.    Elapsed: 0:41:55.\n  Batch 1,000  of  5,623.    Elapsed: 0:43:40.\n  Batch 1,040  of  5,623.    Elapsed: 0:45:24.\n  Batch 1,080  of  5,623.    Elapsed: 0:47:09.\n  Batch 1,120  of  5,623.    Elapsed: 0:48:54.\n  Batch 1,160  of  5,623.    Elapsed: 0:50:39.\n  Batch 1,200  of  5,623.    Elapsed: 0:52:24.\n  Batch 1,240  of  5,623.    Elapsed: 0:54:09.\n  Batch 1,280  of  5,623.    Elapsed: 0:55:53.\n  Batch 1,320  of  5,623.    Elapsed: 0:57:38.\n  Batch 1,360  of  5,623.    Elapsed: 0:59:23.\n  Batch 1,400  of  5,623.    Elapsed: 1:01:08.\n  Batch 1,440  of  5,623.    Elapsed: 1:02:52.\n  Batch 1,480  of  5,623.    Elapsed: 1:04:37.\n  Batch 1,520  of  5,623.    Elapsed: 1:06:22.\n  Batch 1,560  of  5,623.    Elapsed: 1:08:07.\n  Batch 1,600  of  5,623.    Elapsed: 1:09:51.\n  Batch 1,640  of  5,623.    Elapsed: 1:11:36.\n  Batch 1,680  of  5,623.    Elapsed: 1:13:21.\n  Batch 1,720  of  5,623.    Elapsed: 1:15:06.\n  Batch 1,760  of  5,623.    Elapsed: 1:16:50.\n  Batch 1,800  of  5,623.    Elapsed: 1:18:35.\n  Batch 1,840  of  5,623.    Elapsed: 1:20:20.\n  Batch 1,880  of  5,623.    Elapsed: 1:22:05.\n  Batch 1,920  of  5,623.    Elapsed: 1:23:49.\n  Batch 1,960  of  5,623.    Elapsed: 1:25:34.\n  Batch 2,000  of  5,623.    Elapsed: 1:27:19.\n  Batch 2,040  of  5,623.    Elapsed: 1:29:04.\n  Batch 2,080  of  5,623.    Elapsed: 1:30:48.\n  Batch 2,120  of  5,623.    Elapsed: 1:32:33.\n  Batch 2,160  of  5,623.    Elapsed: 1:34:18.\n  Batch 2,200  of  5,623.    Elapsed: 1:36:03.\n  Batch 2,240  of  5,623.    Elapsed: 1:37:47.\n  Batch 2,280  of  5,623.    Elapsed: 1:39:32.\n  Batch 2,320  of  5,623.    Elapsed: 1:41:17.\n  Batch 2,360  of  5,623.    Elapsed: 1:43:02.\n  Batch 2,400  of  5,623.    Elapsed: 1:44:47.\n  Batch 2,440  of  5,623.    Elapsed: 1:46:31.\n  Batch 2,480  of  5,623.    Elapsed: 1:48:16.\n  Batch 2,520  of  5,623.    Elapsed: 1:50:01.\n  Batch 2,560  of  5,623.    Elapsed: 1:51:46.\n  Batch 2,600  of  5,623.    Elapsed: 1:53:31.\n  Batch 2,680  of  5,623.    Elapsed: 1:57:00.\n  Batch 2,720  of  5,623.    Elapsed: 1:58:45.\n  Batch 2,760  of  5,623.    Elapsed: 2:00:30.\n  Batch 2,800  of  5,623.    Elapsed: 2:02:14.\n  Batch 2,840  of  5,623.    Elapsed: 2:03:59.\n  Batch 2,880  of  5,623.    Elapsed: 2:05:44.\n  Batch 2,920  of  5,623.    Elapsed: 2:07:29.\n  Batch 2,960  of  5,623.    Elapsed: 2:09:14.\n  Batch 3,000  of  5,623.    Elapsed: 2:10:58.\n  Batch 3,040  of  5,623.    Elapsed: 2:12:43.\n  Batch 3,080  of  5,623.    Elapsed: 2:14:28.\n  Batch 3,120  of  5,623.    Elapsed: 2:16:13.\n  Batch 3,160  of  5,623.    Elapsed: 2:17:57.\n  Batch 3,200  of  5,623.    Elapsed: 2:19:42.\n  Batch 3,240  of  5,623.    Elapsed: 2:21:27.\n  Batch 3,280  of  5,623.    Elapsed: 2:23:12.\n  Batch 3,320  of  5,623.    Elapsed: 2:24:56.\n  Batch 3,360  of  5,623.    Elapsed: 2:26:41.\n  Batch 3,400  of  5,623.    Elapsed: 2:28:26.\n  Batch 3,440  of  5,623.    Elapsed: 2:30:11.\n  Batch 3,480  of  5,623.    Elapsed: 2:31:55.\n  Batch 3,520  of  5,623.    Elapsed: 2:33:40.\n  Batch 3,560  of  5,623.    Elapsed: 2:35:25.\n  Batch 3,600  of  5,623.    Elapsed: 2:37:10.\n  Batch 3,640  of  5,623.    Elapsed: 2:38:55.\n  Batch 3,680  of  5,623.    Elapsed: 2:40:39.\n  Batch 3,720  of  5,623.    Elapsed: 2:42:24.\n  Batch 3,760  of  5,623.    Elapsed: 2:44:09.\n  Batch 3,800  of  5,623.    Elapsed: 2:45:54.\n  Batch 3,840  of  5,623.    Elapsed: 2:47:39.\n  Batch 3,880  of  5,623.    Elapsed: 2:49:24.\n  Batch 3,920  of  5,623.    Elapsed: 2:51:09.\n  Batch 3,960  of  5,623.    Elapsed: 2:52:53.\n  Batch 4,000  of  5,623.    Elapsed: 2:54:38.\n  Batch 4,040  of  5,623.    Elapsed: 2:56:23.\n  Batch 4,080  of  5,623.    Elapsed: 2:58:08.\n  Batch 4,120  of  5,623.    Elapsed: 2:59:52.\n  Batch 4,160  of  5,623.    Elapsed: 3:01:37.\n  Batch 4,200  of  5,623.    Elapsed: 3:03:22.\n  Batch 4,240  of  5,623.    Elapsed: 3:05:07.\n  Batch 4,280  of  5,623.    Elapsed: 3:06:52.\n  Batch 4,320  of  5,623.    Elapsed: 3:08:36.\n  Batch 4,360  of  5,623.    Elapsed: 3:10:21.\n  Batch 4,400  of  5,623.    Elapsed: 3:12:06.\n  Batch 4,440  of  5,623.    Elapsed: 3:13:51.\n  Batch 4,480  of  5,623.    Elapsed: 3:15:36.\n  Batch 4,520  of  5,623.    Elapsed: 3:17:20.\n  Batch 4,560  of  5,623.    Elapsed: 3:19:05.\n  Batch 4,600  of  5,623.    Elapsed: 3:20:50.\n  Batch 4,640  of  5,623.    Elapsed: 3:22:35.\n  Batch 4,680  of  5,623.    Elapsed: 3:24:20.\n  Batch 4,720  of  5,623.    Elapsed: 3:26:05.\n  Batch 4,760  of  5,623.    Elapsed: 3:27:49.\n  Batch 4,800  of  5,623.    Elapsed: 3:29:34.\n  Batch 4,840  of  5,623.    Elapsed: 3:31:19.\n  Batch 4,880  of  5,623.    Elapsed: 3:33:04.\n  Batch 4,920  of  5,623.    Elapsed: 3:34:49.\n  Batch 4,960  of  5,623.    Elapsed: 3:36:34.\n  Batch 5,000  of  5,623.    Elapsed: 3:38:18.\n  Batch 5,040  of  5,623.    Elapsed: 3:40:03.\n  Batch 5,080  of  5,623.    Elapsed: 3:41:48.\n  Batch 5,120  of  5,623.    Elapsed: 3:43:33.\n  Batch 5,160  of  5,623.    Elapsed: 3:45:17.\n  Batch 5,200  of  5,623.    Elapsed: 3:47:02.\n  Batch 5,240  of  5,623.    Elapsed: 3:48:47.\n  Batch 5,280  of  5,623.    Elapsed: 3:50:32.\n  Batch 5,320  of  5,623.    Elapsed: 3:52:16.\n  Batch 5,360  of  5,623.    Elapsed: 3:54:01.\n  Batch 5,400  of  5,623.    Elapsed: 3:55:46.\n  Batch 5,440  of  5,623.    Elapsed: 3:57:31.\n  Batch 5,480  of  5,623.    Elapsed: 3:59:15.\n  Batch 5,520  of  5,623.    Elapsed: 4:01:00.\n  Batch 5,560  of  5,623.    Elapsed: 4:02:45.\n  Batch 5,600  of  5,623.    Elapsed: 4:04:30.\n\n  Average training loss: 0.14\n  Training epoch took: 4:05:28\n\nRunning Validation...\n  Accuracy: 0.97\n  Validation Loss: 0.14\n  Validation took: 0:07:34\n\nTraining complete!\nTotal training took 8:26:09 (h:mm:ss)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\nfor batch in val_loader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    with torch.no_grad():\n        outputs = model(**batch)\n    \n    logits = outputs.logits\n    logits = logits.detach().cpu().numpy()\n    label_ids = batch['labels'].to('cpu').numpy()\n    \n    preds = np.argmax(logits, axis=1)\n    \n    predictions.extend(preds)\n    true_labels.extend(label_ids)\n\n# Classification report\nprint(classification_report(true_labels, predictions))\n\n# Confusion matrix\nconf_mat = confusion_matrix(true_labels, predictions)\nprint(\"Confusion Matrix:\")\nprint(conf_mat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T15:03:55.604983Z","iopub.execute_input":"2024-11-10T15:03:55.605758Z","iopub.status.idle":"2024-11-10T15:11:30.000754Z","shell.execute_reply.started":"2024-11-10T15:03:55.605717Z","shell.execute_reply":"2024-11-10T15:11:29.999759Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       130\n           1       0.97      1.00      0.99      4868\n\n    accuracy                           0.97      4998\n   macro avg       0.49      0.50      0.49      4998\nweighted avg       0.95      0.97      0.96      4998\n\nConfusion Matrix:\n[[   0  130]\n [   0 4868]]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import os\n\n# Define the output directory for saving\noutput_dir = '/kaggle/working/model_save/'  # Save in working directory\n\n# Create the directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Saving model to {output_dir}\")\n\n# Save the trained model, configuration, and tokenizer\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T15:13:41.512024Z","iopub.execute_input":"2024-11-10T15:13:41.512768Z","iopub.status.idle":"2024-11-10T15:13:46.632390Z","shell.execute_reply.started":"2024-11-10T15:13:41.512729Z","shell.execute_reply":"2024-11-10T15:13:46.631400Z"}},"outputs":[{"name":"stdout","text":"Saving model to /kaggle/working/model_save/\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/model_save/tokenizer_config.json',\n '/kaggle/working/model_save/special_tokens_map.json',\n '/kaggle/working/model_save/sentencepiece.bpe.model',\n '/kaggle/working/model_save/added_tokens.json')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_huggingface_token' with your actual token\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:08:39.583613Z","iopub.execute_input":"2024-11-13T14:08:39.584010Z","iopub.status.idle":"2024-11-13T14:08:39.729551Z","shell.execute_reply.started":"2024-11-13T14:08:39.583967Z","shell.execute_reply":"2024-11-13T14:08:39.728481Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import HfApi, upload_folder\n\n# Define the repository name and your model folder\nrepo_name = \"shafitanvir31/bangla-Roberta-xlm-finetuned\"  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:09:12.262924Z","iopub.execute_input":"2024-11-13T14:09:12.263304Z","iopub.status.idle":"2024-11-13T14:09:12.267833Z","shell.execute_reply.started":"2024-11-13T14:09:12.263270Z","shell.execute_reply":"2024-11-13T14:09:12.266892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_path = \"/kaggle/working/model_save\"\n\n# Create a repository\napi = HfApi()\napi.create_repo(repo_id=repo_name, private=False)\n\n# Upload the entire model folder to the repository\nupload_folder(\n    folder_path=model_path,\n    repo_id=repo_name,\n    commit_message=\"Upload fine-tuned Bangla BERT model\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T14:09:35.060111Z","iopub.execute_input":"2024-11-13T14:09:35.060627Z","iopub.status.idle":"2024-11-13T14:10:44.202647Z","shell.execute_reply.started":"2024-11-13T14:09:35.060567Z","shell.execute_reply":"2024-11-13T14:10:44.201757Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5bfb2100ba45d18d31b6050fdf8503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d90f6a68114e058e928b5eef5c7f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45a279c674f42e6abb91980c8398d2b"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shafitanvir31/bangla-Roberta-xlm-finetuned/commit/87c83b803ad4f73768f4076a2825f7fec945ab5b', commit_message='Upload fine-tuned Bangla BERT model', commit_description='', oid='87c83b803ad4f73768f4076a2825f7fec945ab5b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shafitanvir31/bangla-Roberta-xlm-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='shafitanvir31/bangla-Roberta-xlm-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":3}]}